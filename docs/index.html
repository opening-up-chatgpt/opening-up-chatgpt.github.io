<!DOCTYPE html>

<html>
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Opening up ChatGPT: LLM openness leaderboard</title>
<link href="styles.css" rel="stylesheet"/>
<link href="favicon.png" rel="icon" type="image/x-icon"/>
</head>
<body>
<div id="header">
<h1><a href="" title="Opening up ChatGPT: tracking openness, transparency, and accountability in instruction-tuned text generators (2022-2024)"><img alt="Opening up ChatGPT" id="title-logo" src="logos/openchatgpt-logo-favicon-red-on-transparent.png"/>Opening up ChatGPT</a>: tracking openness of instruction-tuned LLMs</h1>
</div>
<div id="content">
<div class="highlight" id="citation"><p>Superseded by the <strong><a href="https://osai-index.eu">European Open Source AI Index</a></strong> | The table below is provided for historical purposes but is <strong>no longer updated</strong>. We have tripled the amount of models and are including code, audio, and image models at <a href="https://osai-index.eu">osai-index.eu</a>.</p>
</div>
<p id="tagline">There is a growing amount of instruction-tuned text generators billing themselves as 'open source'. How open are they really? <span class="link-icon">🔗</span><a href="https://dl.acm.org/doi/10.1145/3630106.3659005" target="_blank">FAccT'24</a> <span class="link-icon">🔗</span><a href="https://doi.org/10.1145/3571884.3604316" target="_blank">CUI'23</a></p>
<div id="included-table"><table>
<thead>
<tr class="main-header"><th>Project</th><th colspan="6">Availability</th><th colspan="6">Documentation</th><th colspan="2">Access</th></tr>
<tr class="second-header"><th>(maker, bases, URL)</th><th>Open code</th><th>LLM data</th><th>LLM weights</th><th>RL data</th><th>RL weights</th><th>License</th><th>Code</th><th>Architecture</th><th>Preprint</th><th>Paper</th><th>Modelcard</th><th>Datasheet</th><th>Package</th><th>API</th></tr>
</thead>
<tbody>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/olmo-7b-instruct.yaml" target="_blank" title="data: olmo-7b-instruct.yaml">OLMo 7B Instruct</a></td><td class="open data-cell"><a href="https://github.com/allenai/OLMo" target="_blank" title="Multiple repos with training, architecture and fine-tuning code available">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/allenai/dolma" target="_blank" title="Dolma training data released and documented in exemplary way">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778" target="_blank" title="OLMo 7B and many training checkpoints available">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned" target="_blank" title="Instruction tuning datasets documented and made available in exemplary ways">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-Instruct/tree/main" target="_blank" title="Full model weights made available">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-Instruct" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-Instruct#model-sources" target="_blank" title="repositories and code well-described, commented and documented">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-Instruct#model-sources" target="_blank" title="Architectured documented in detail in model card, preprint, and technical blog posts">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2402.00838" target="_blank" title="Preprint describes model architecture, training and fine-tuning data, and training and SFT pipelines">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-Instruct" target="_blank" title="Model card provides broad overview and links to full details">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/allenai/dolma" target="_blank" title="Data sheets and documentation available for the datasets used, linked here is Dolma">✔︎</a></td><td class="open data-cell"><a href="https://pypi.org/project/ai2-olmo/" target="_blank" title="AI2-olmo available on PyPi">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/allenai/OLMo-7B-hf" target="_blank" title="Available through HuggingFace though model is too large to run on free inference API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://blog.allenai.org/olmo-open-language-model-87ccfc95f580" target="_blank" title="">Ai2</a></td><td class="llmbase" colspan="3">LLM base: OLMo 7B</td><td class="rlbase" colspan="3">RL base: OpenInstruct</td><td colspan="7"></td><td class="source-link"><a href="https://allenai.org" target="_blank" title="Ai2">12.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/bloomz.yaml" target="_blank" title="data: bloomz.yaml">BLOOMZ</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Repository provides a guided overview to all components">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#data" target="_blank" title="Data made available &amp; documented in detail in repo and preprint">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#models" target="_blank" title="Model made available on github">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/bigscience/xP3all" target="_blank" title="From the documentation 'xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts &amp; datasets across 46 of languages &amp; 16 NLP tasks'">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/bigscience/bloomz-optimizer-states/tree/main" target="_blank" title="Fine-tuned checkpoint available for download">~</a></td><td class="partial data-cell"><a href="https://bigscience.huggingface.co/blog/the-bigscience-rail-license" target="_blank" title="Code licensed under Apache 2.0, model under bespoke 'Responsible AI License' which imposes some limitations">~</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf" target="_blank" title="Code well documented and actively maintained">✔︎</a></td><td class="open data-cell"><a href="https://github.com/bigscience-workshop/xmtf#create-xp3x" target="_blank" title="Architecture described in preprint, code available in github repo, recipe on HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2211.05100" target="_blank" title="Preprint (updated June 2023) of 65 pages + 10 page appendix">✔︎</a></td><td class="open data-cell"><a href="https://aclanthology.org/2023.acl-long.891/" target="_blank" title="Peer-reviewed paper of 9 pages + 114 page appendix describes the multitask finetuning (instruction tuning) of BLOOM (see preprint) to form BLOOMZ">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/bigscience/bloomz" target="_blank" title="Model card">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/bigscience/xP3" target="_blank" title="Dataset documented in dataset card at HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No packages published">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/spaces/bigscience/petals-api" target="_blank" title="Petals API via HuggingFace not always available ('not enough hardware capacity')">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/bigscience/bloomz" target="_blank" title="">bigscience-workshop</a></td><td class="llmbase" colspan="3">LLM base: BLOOMZ, mT0</td><td class="rlbase" colspan="3">RL base: xP3</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/bigscience-workshop" target="_blank" title="bigscience-workshop">12.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/amber.yaml" target="_blank" title="data: amber.yaml">AmberChat</a></td><td class="open data-cell"><a href="https://github.com/LLM360/amber-train/tree/main" target="_blank" title="amber-train repository includes code for training and finetuning.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/LLM360/AmberDatasets" target="_blank" title="data well-documented and openly available">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/LLM360/Amber" target="_blank" title="360 model checkpoints released">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="RL and fine-tuning data shared and documented">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/LLM360/AmberChat" target="_blank" title="Finetuned model available for download.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/LLM360/AmberChat" target="_blank" title="Everything licensed under Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/LLM360" target="_blank" title="Code documented in helpful readme.md files but only partly inline.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2312.06550" target="_blank" title="Architecture described in preprint, but not all details documented.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2312.06550" target="_blank" title="Preprint describes architecture, design choices, training and fine-tuning.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper yet.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/LLM360/AmberChat" target="_blank" title="Model card doesn't specify use or limitations">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/LLM360/AmberDatasets" target="_blank" title="Concise description (better than most), but doesn't specify funders, purposes, representativeness, legal status as prescribed by datasheets industry standard">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No released package found">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/LLM360/AmberChat" target="_blank" title="Free Huggingface inference API.">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/LLM360/AmberChat" target="_blank" title="">LLM360</a></td><td class="llmbase" colspan="3">LLM base: Amber</td><td class="rlbase" colspan="3">RL base: ShareGPT + Evol-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://www.llm360.ai/index.html" target="_blank" title="LLM360">10.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Open-Assistant.yaml" target="_blank" title="data: Open-Assistant.yaml">Open Assistant</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" title="Code includes guide for developers">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/data/datasets" target="_blank" title="Datasets documented in detail and recipes for cleaning up and downloading provided in code notebooks.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/OpenAssistant" target="_blank" title="Model weights in several variants downloadable through HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/OpenAssistant/oasst1" target="_blank" title="OpenAssistant Conversations is 'a human-generated, human-annotated assistant-style conversation corpus consisting of 161443 messages distributed across 66497 conversation trees, in 35 different languages, annotated with 461292 quality ratings' (preprint)">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF weights not separately released">✘</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/faq#what-license-does-open-assistant-use" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/docs/intro" target="_blank" title="Separate website provides entry point to comprehensive documentation">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LAION-AI/Open-Assistant/tree/main/model" target="_blank" title="Instructions to tune the pipeline on training data">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs//2304.07327" target="_blank" title="Preprint describes creation of OpenAssistant Conversations corpus for instruction tuning, but not the base LLM, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://projects.laion.ai/Open-Assistant/api" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://open-assistant.io/" target="_blank" title="">LAION-AI</a></td><td class="llmbase" colspan="3">LLM base: Pythia 12B</td><td class="rlbase" colspan="3">RL base: OpenAssistant Conversations</td><td colspan="7"></td><td class="source-link"><a href="https://open-assistant.io/" target="_blank" title="LAION-AI">9.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/OpenChat.yaml" target="_blank" title="data: OpenChat.yaml">OpenChat 3.5 7B</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/tree/master/ochat" target="_blank" title="Repository offers a large amount of fairly well-organized code for data curation and model">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Pretraining data for Mistral is nowhere disclosed or documented">✘</a></td><td class="open data-cell"><a href="https://github.com/mistralai/mistral-src#download-the-model" target="_blank" title="Mistral 7B weights available via Mistral repository">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Preprint says shareGPT dataset 'collected from sharegpt.com' but not disclosed or made available by this project">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/openchat/openchat_3.5/tree/main" target="_blank" title="Instruction-tuned model weights made available via HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/blob/master/LICENSE" target="_blank" title="Code and model released under Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/imoneoi/openchat/tree/master/ochat" target="_blank" title="There is plenty of code in the github repository but only some of it is documented">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2309.11235" target="_blank" title="Architecture quite well described in preprint">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2309.11235" target="_blank" title="Preprint describes the model architecture and instruction tuning approach, though is hampered by building on notoriously closed Llama2">✔︎</a></td><td class="open data-cell"><a href="https://openreview.net/forum?id=AOJyfhWYHf" target="_blank" title="Paper reviewed and accepted for ICLR 2024">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/openchat/openchat_v3.2" target="_blank" title="There is a model card that provides some details on architecture and evaluation">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="open data-cell"><a href="https://github.com/imoneoi/openchat/tree/master#installation" target="_blank" title="Python package 'ochat' provided through pip">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Model too large to load onto HuggingFace free inference API, so only available through Inference Endpoints or package">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/imoneoi/openchat" target="_blank" title="">Tshinghua University</a></td><td class="llmbase" colspan="3">LLM base: Mistral 7B</td><td class="rlbase" colspan="3">RL base: ShareGPT with C-RLFT</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/imoneoi" target="_blank" title="Tshinghua University">9.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/pythia-chat-base-7B.yaml" target="_blank" title="data: pythia-chat-base-7B.yaml">Pythia-Chat-Base-7B-v0.16</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenDataHub" target="_blank" title="Training data curated and shared in separate repository">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B/tree/main" target="_blank" title="Model weights available via HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="From the documentation 'This is our attempt to create a large instruction dataset of medium quality along with a smaller high quality instruciton dataset (OIG-small-chip2).'">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not separately made available">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B#model-details" target="_blank" title="Apache 2.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="open data-cell"><a href="https://github.com/togethercomputer/OpenChatKit#reproducing-pythia-chat-base-7b" target="_blank" title="Architecture and recipe for reproducing model provided">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="Preprint describes LM base (Pythia) but not instruction tuning details">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="Model card partially available but fairly minimally specified">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/laion/OIG" target="_blank" title="OIG instruction dataset documented">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B" target="_blank" title="">togethercomputer</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: OIG</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/togethercomputer" target="_blank" title="togethercomputer">9.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/cerebras-gpt-111m-instruction.yaml" target="_blank" title="data: cerebras-gpt-111m-instruction.yaml">Cerebras GPT 111M Instruction</a></td><td class="partial data-cell"><a href="https://github.com/Cerebras/gigaGPT" target="_blank" title="Some of the training code available in GigaGPT, but fine-tuning">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/EleutherAI/pile" target="_blank" title="Eleuther AI's The Pile">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/cerebras/Cerebras-GPT-111M" target="_blank" title="base model available via Cerebras">✔︎</a></td><td class="open data-cell"><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json" target="_blank" title="Alpaca GPT4">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction/tree/main" target="_blank" title="Finetuned model weights available">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction/" target="_blank" title="Licensing situation unclear as model page mentions no license (base model is licensed Apache 2.0)">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only sparsely documented">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="Described in preprint">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.03208" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/cerebras/Cerebras-GPT-111M" target="_blank" title="Only serves as as advertising for the model">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2201.07311" target="_blank" title="Datasheet available for The Pile">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="Available through HuggingFace inference API">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction" target="_blank" title="">Cerebras + Schramm</a></td><td class="llmbase" colspan="3">LLM base: Cerebras</td><td class="rlbase" colspan="3">RL base: Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/Cerebras" target="_blank" title="Cerebras + Schramm">8.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/RedPajama-INCITE-Instruct-7B.yaml" target="_blank" title="data: RedPajama-INCITE-Instruct-7B.yaml">RedPajama-INCITE-Instruct-7B</a></td><td class="partial data-cell"><a href="https://github.com/togethercomputer/redpajama.cpp/tree/master/examples/redpajama" target="_blank" title="Code for datasets made available in exemplary ways; code for training and tuning harder to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T" target="_blank" title="RedPajama-Data-1T made available on HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base" target="_blank" title="Base is RedPajama-INCITE-7B-Base">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct" target="_blank" title="The model was trained on a large collection of diverse data, including Chain-of-Thought (CoT), Public Pool of Prompts (P3) dataset, Natural-Instructions (NI) dataset.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Instruction-tuned version made available in paralle with base version">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct/blob/main/README.md" target="_blank" title="Models licensed under Apache 2.0, but note that the data itself is variably licensed and so imposes some limitations.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code for base LLM and instruction tuning datasets beautifully documented; code specifying training and fine-tuning sparsely documented.">~</a></td><td class="partial data-cell"><a href="https://together.ai/blog/redpajama" target="_blank" title="Architecture detailed on model card, crucial parts appear to be forked from GPT-NeoX">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Model card and readme provide details on datasets and">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T" target="_blank" title="Data sheet includes links to data and recipes to create from scratch">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No separate package found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="Hosted inference API available through HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct" target="_blank" title="">TogetherComputer</a></td><td class="llmbase" colspan="3">LLM base: RedPajama-INCITE-7B-Base</td><td class="rlbase" colspan="3">RL base: various (GPT-JT recipe)</td><td colspan="7"></td><td class="source-link"><a href="https://together.ai/" target="_blank" title="TogetherComputer">8.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/dolly.yaml" target="_blank" title="data: dolly.yaml">dolly</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2304.01373" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/databrickslabs/dolly" target="_blank" title="">databricks</a></td><td class="llmbase" colspan="3">LLM base: EleutherAI pythia</td><td class="rlbase" colspan="3">RL base: databricks-dolly-15k</td><td colspan="7"></td><td class="source-link"><a href="https://www.databricks.com" target="_blank" title="databricks">8.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/tulu-2-70b.yaml" target="_blank" title="data: tulu-2-70b.yaml">Tulu V2 DPO 70B</a></td><td class="open data-cell"><a href="https://github.com/allenai/open-instruct" target="_blank" title="Important effort to make available fine-tuning procedure and source code. Not seen a repository for base model training yet.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on Llama2 so nothing known about training data">✘</a></td><td class="partial data-cell"><a href="https://github.com/meta-llama/" target="_blank" title="Base model made available via Meta (requires privacy-defying signup)">~</a></td><td class="open data-cell"><a href="https://github.com/allenai/open-instruct" target="_blank" title="Codebase used for fine-tuning this model and made available for others">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/allenai/tulu-2-dpo-70b/tree/main" target="_blank" title="Fine-tuned weights available">✔︎</a></td><td class="partial data-cell"><a href="https://allenai.org/impact-license" target="_blank" title="Meta Community License and AI2 Impact license">~</a></td><td class="partial data-cell"><a href="https://github.com/allenai/open-instruct" target="_blank" title="OpenInstruct code well-documented">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" target="_blank" title="Post-llama2 aspects quite well described in papers, e.g. DPO based on Zephyr Beta, but base model is irrevocably closed">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2311.10702" target="_blank" title="Preprint covers the training of Tulu2">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed work found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/allenai/tulu-2-70b" target="_blank" title="Model card offers little details">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture" target="_blank" title="Some datasheets available (for instruction tuning), but base model entirely undocumented.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No separate package found">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="Available via various APIs">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/allenai/tulu-2-dpo-70b" target="_blank" title="A new DPO model from AllenAI called Tülu">AllenAI</a></td><td class="llmbase" colspan="3">LLM base: Llama2</td><td class="rlbase" colspan="3">RL base: Tulu SFT, Ultrafeedback</td><td colspan="7"></td><td class="source-link"><a href="https://allenai.org/" target="_blank" title="AllenAI">8.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/MPT-30b-instruct.yaml" target="_blank" title="data: MPT-30b-instruct.yaml">MPT-30B Instruct</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt" target="_blank" title="Codebase part of LLM foundry">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/c4" target="_blank" title="C4 is part of the dataset but a precise specification of source data is hard to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct/tree/main" target="_blank" title="Weights available via HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/mosaicml/dolly_hhrlhf" target="_blank" title="dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="CC-by-SA 3.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/" target="_blank" title="LLM Foundry codebase is well-documented and in active development.">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="Architecture reasonably well-documented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="Modelcard is somewhat lacking in detail">~</a></td><td class="closed data-cell"><a href="https://www.mosaicml.com/blog/mpt-30b" target="_blank" title="Datasheet not available; data somewhat documented in blog post at link">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="API via HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/mosaicml/mpt-30b-instruct" target="_blank" title="">MosaicML</a></td><td class="llmbase" colspan="3">LLM base: MosaicML</td><td class="rlbase" colspan="3">RL base: dolly, anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://www.mosaicml.com" target="_blank" title="MosaicML">7.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/MPT-7b-instruct.yaml" target="_blank" title="data: MPT-7b-instruct.yaml">MPT-7B Instruct</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/tree/main/llmfoundry/models/mpt" target="_blank" title="Codebase part of LLM foundry">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/c4" target="_blank" title="C4 is part of the dataset but a precise specification of source data is hard to find">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct/tree/main" target="_blank" title="Weights available via HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/mosaicml/dolly_hhrlhf" target="_blank" title="dolly-hhrlhf, combination of Databrick dolly-15k dataset and a filtered subset of Anthropic HH-RLHF">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="CC-by-SA 3.0">✔︎</a></td><td class="open data-cell"><a href="https://github.com/mosaicml/llm-foundry/" target="_blank" title="LLM Foundry codebase is well-documented and in active development">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="Architecture reasonably well-documented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/mosaicml/mpt-7b-instruct" target="_blank" title="">MosaicML</a></td><td class="llmbase" colspan="3">LLM base: MosaicML</td><td class="rlbase" colspan="3">RL base: dolly, anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://www.mosaicml.com" target="_blank" title="MosaicML">7.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/trlx.yaml" target="_blank" title="data: trlx.yaml">trlx</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/CarperAI/trlx" target="_blank" title="">carperai</a></td><td class="llmbase" colspan="3">LLM base: various (pythia, flan, OPT)</td><td class="rlbase" colspan="3">RL base: various</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/CarperAI/trlx" target="_blank" title="carperai">7.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/neuralchat-7b.yaml" target="_blank" title="data: neuralchat-7b.yaml">NeuralChat 7B</a></td><td class="partial data-cell"><a href="https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3" target="_blank" title="Mistral base model is not open, but repo gives sample code for fine-tuning based on that">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Mistral has not disclosed anything about its training data">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" target="_blank" title="Based on Mistral 7B 0.1">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/Open-Orca/SlimOrca" target="_blank" title="RL dataset used for post-training is shared and available">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/Intel/neural-chat-7b-v3-1/tree/main" target="_blank" title="finetuned model made openly available by Intel">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/Intel/neural-chat-7b-v3-1/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3#how-to-train-intelneural-chat-7b-v3-1-on-intel-gaudi2" target="_blank" title="Mistral remains closed so only documentation pertains to fine-tuning steps, but that code is quite well documented, so partial.">~</a></td><td class="partial data-cell"><a href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3" target="_blank" title="Described in on HuggingFace model card and a Medium post">~</a></td><td class="closed data-cell"><a href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3" target="_blank" title="A medium post is apparently the only scientific documentation of this model">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/Intel/neural-chat-7b-v3-1" target="_blank" title="No model card for Mistral base model portion.">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/Open-Orca/OpenOrca" target="_blank" title="SlimOrca dataset is described as part of OpenOrca (but partial since Mistral data is incrutable)">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/Intel/neural-chat-7b-v3-1#fp32-inference-with-transformers" target="_blank" title="Code for running with transformers provided by Intel">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Provided through HuggingFace but model too large to run via inference API, local deployment or paid access needed">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/Intel/neural-chat-7b-v3-1" target="_blank" title="A mistral-based Orca-finetuned chat model">Intel</a></td><td class="llmbase" colspan="3">LLM base: Mistral 7B</td><td class="rlbase" colspan="3">RL base: Orca</td><td colspan="7"></td><td class="source-link"><a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/NeuralChat-A-Customizable-Chatbot-Framework/post/1526789" target="_blank" title="Intel">7.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/vicuna13B-lmsys.yaml" target="_blank" title="data: vicuna13B-lmsys.yaml">Vicuna 13B v 1.3</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Actively maintained repository">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset" target="_blank" title="Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="Unlike Vicuna 13B v0, these weights do not require applying delta">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.'">✘</a></td><td class="closed data-cell"><a href="https://github.com/lm-sys/FastChat#fine-tuning" target="_blank" title="No model weights are shared for the instruction tuning">✘</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" title="From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.'">~</a></td><td class="open data-cell"><a href="https://github.com/lm-sys/FastChat" target="_blank" title="Code is quite well-documented and released as part of the FastChat framework.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05685.pdf" target="_blank" title="Preprint covers training of the Vicuna model.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Minimal model card, but many details are not provided or have to be pieced together from elsewhere.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet provided.">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/fschat/0.1.2/" target="_blank" title="Available via pip">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat#api" target="_blank" title="Support provided for several APIs OpenAI restful, HuggingFace, Langchain">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/lmsys/vicuna-13b-v1.3" target="_blank" title="Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.">LMSYS</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: ShareGPT</td><td colspan="7"></td><td class="source-link"><a href="https://lmsys.org/" target="_blank" title="LMSYS">7.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/minChatGPT.yaml" target="_blank" title="data: minChatGPT.yaml">minChatGPT</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="hhttps://github.com/ethanyanjiali/minChatGPT" target="_blank" title="">ethanyanjiali</a></td><td class="llmbase" colspan="3">LLM base: GPT2</td><td class="rlbase" colspan="3">RL base: anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/ethanyanjiali/minChatGPT" target="_blank" title="ethanyanjiali">7.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ChatRWKV.yaml" target="_blank" title="data: ChatRWKV.yaml">ChatRWKV</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="Various community-contributed enhancements available">✔︎</a></td><td class="partial data-cell"><a href="https://pile.eleuther.ai/" target="_blank" title="Trained on The Pile. Recent versions also build on Red Pajama (https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)">~</a></td><td class="open data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-world/tree/main" target="_blank" title="Model weights released across different HuggingFace spaces">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Instruction tuning data not separately available. Documentation 'These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more'">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Weights not separately available.">✘</a></td><td class="open data-cell"><a href="https://github.com/BlinkDL/ChatRWKV/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code documentation scattered across github repo and HuggingFace spaces">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Architecture described in preprint (LM part) but not all details clearly documented.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2305.13048" target="_blank" title="Preprint covers only LLM (RNN based), not instruction fine-tuning, so partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or published data audit known">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No modelcard, HuggingFace spaces only used to share files">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" title="No data sheet, HuggingFac spaces only used to share files">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/rwkv/" target="_blank" title="Available through pip install rwkv">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="API via HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/BlinkDL/ChatRWKV" target="_blank" title="">BlinkDL/RWKV</a></td><td class="llmbase" colspan="3">LLM base: RWKV-LM</td><td class="rlbase" colspan="3">RL base: alpaca, shareGPT (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://www.rwkv.com/" target="_blank" title="BlinkDL/RWKV">6.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/BELLE.yaml" target="_blank" title="data: BELLE.yaml">BELLE</a></td><td class="open data-cell"><a href="https://github.com/LianjiaTech/BELLE" target="_blank" title="Repository contains a fair bit of code">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Open for variants based on BLOOMZ. Closed for variants based on LLaMA, whose pretraining data is nowhere disclosed or documented.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based but copyright status unclear">~</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M" target="_blank" title="Synthetic BELLE training data in Chinese released in batches">~</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/tree/main/models" target="_blank" title="Some models available, most only as delta weights requiring separate access to LLaMA">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Lowest common denominator is non-OSI approved LLaMA licence agreement">✘</a></td><td class="partial data-cell"><a href="https://github.com/LianjiaTech/BELLE/blob/main/README_en.md" target="_blank" title="Quite some documentation on Github, though not all well-organized">~</a></td><td class="open data-cell"><a href="https://github.com/LianjiaTech/BELLE/blob/main/README_en.md" target="_blank" title="Specified in a fair bit of detail on github">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2303.14742" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card found">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="No data sheet found">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No dedicated package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API found">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/LianjiaTech/BELLE" target="_blank" title="">KE Technologies</a></td><td class="llmbase" colspan="3">LLM base: LLaMA &amp; BLOOMZ</td><td class="rlbase" colspan="3">RL base: alpaca, shareGPT, Belle (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="http://www.ke.com" target="_blank" title="KE Technologies">6.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/geitje-ultra.yaml" target="_blank" title="data: geitje-ultra.yaml">Geitje Ultra 7B</a></td><td class="closed data-cell"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra#training-procedure" target="_blank" title="Mistral has limited source code available. No training code for Geitje found apart from the recipe used with the alignment handbook.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/Rijgersberg/GEITje-7B#geitje--trained-further-on-dutch-texts" target="_blank" title="Mistral provides no documentation of any of its pretraining data. Geitje Ultra 7B is based on Geitje 7B, which does disclose that Dutch pretraining data includes Gigacorpus and MADLAD.">~</a></td><td class="open data-cell"><a href="https://github.com/mistralai/mistral-src#download-the-model" target="_blank" title="Mistral base model is available for downloading">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch" target="_blank" title="Ultrafeedback Dutch (synthetic)">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra/tree/main" target="_blank" title="Instruction-tuned model made available through HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra" target="_blank" title="Licensed as CC-BY-ND-4.0 on HuggingFace, though no specific license file or statement found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Only limited code repositories and no clear centralized documentation of code">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra" target="_blank" title="Some information on architecture provided in github repo and HF model card. Training was done using alignment handbook.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2312.12852v1" target="_blank" title="Preprint documents Dutch language resources but architecture and scientific documentation otherwise lacking due to Mistral base">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra" target="_blank" title="Modelcard on HF provides information on fine-tuning but nothing for the Mistral base LLM">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch" target="_blank" title="Datasheet available for DPO and for the Dutch portions of pretraining data, but not for original Mistral pretraining data, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Model available through HuggingFace API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra" target="_blank" title="Dutch instruction-tuned model based on Mistral 7B">Bram van Roy</a></td><td class="llmbase" colspan="3">LLM base: Mistral 7B</td><td class="rlbase" colspan="3">RL base: Ultrafeedback Dutch (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://huggingface.co/BramVanroy/GEITje-7B-ultra" target="_blank" title="Bram van Roy">6.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/phi-3-instruct.yaml" target="_blank" title="data: phi-3-instruct.yaml">Phi 3 Instruct</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code found for pretraining, posttraining, or evaluation">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasets made available and no information on datasets disclosed except very generic claims about filtering for high quality.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No base model of the instruction-tuned Phi-3 was released">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No post-training datasets made available and no information on datasets disclosed except very generic claims about filtering for high quality.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/tree/main" target="_blank" title="Instruction-tuned model weights shared through HuggingFace">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/LICENSE" target="_blank" title="MIT License">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code, so no documentation of source code found">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2404.14219" target="_blank" title="Architecture described in model card and preprint">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2404.14219" target="_blank" title="Preprint describes model architecture but not training data, focusing mostly on benchmarks and evalution">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/LICENSE" target="_blank" title="Model card provides reasonable level of detail">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet made available">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Available through development version of transformers">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Available through HuggingFace API">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct" target="_blank" title="">Microsoft</a></td><td class="llmbase" colspan="3">LLM base: Phi3</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct" target="_blank" title="Microsoft">6.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/wizardlm-13B.yaml" target="_blank" title="data: wizardlm-13B.yaml">WizardLM 13B v1.2</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Fast-evolving repository contains WizardLM code">~</a></td><td class="closed data-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml" target="_blank" title="Based on LLaMA2, which is claimed to be public but nowhere exactly documented.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Based on LLaMA2 weights, which are made conditionally available by Meta.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="The Evol-Instruct V2 dataset contains 196k instruction-following sequences generated from Evol-Instruct">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Model weights offered in HuggingFace repository">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE" target="_blank" title="Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Code is only partially documented, not clearly versioned, and appears to be in flux.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Architecture described in preprint and partly accessible in code repository">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Model card is only a placeholder and generates an error (missing yaml metadata)">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="Dataset card for Evol-Instruct generates an error">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank" title="Empowering Large Pre-Trained Language Models to Follow Complex Instructions">Microsoft &amp; Peking University</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2-13B</td><td class="rlbase" colspan="3">RL base: Evol-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/nlpxucan" target="_blank" title="Microsoft &amp; Peking University">6.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/airoboros.yaml" target="_blank" title="data: airoboros.yaml">Airoboros L2 70B GPT4</a></td><td class="partial data-cell"><a href="https://gist.github.com/jondurbin/87fc040b92a3073125ed516b04bc6e19" target="_blank" title="Repo exists for RL data but only a gist exists for model training and architecture">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Llama2 training data is nowhere documented or disclosed">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Llama2, made conditionally available by Meta">~</a></td><td class="open data-cell"><a href="https://github.com/jondurbin/airoboros" target="_blank" title="Airoboros, an implementation of the Self-Instruct paper">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/tree/main" target="_blank" title="Made available through HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1#licence-and-usage-restrictions" target="_blank" title="Licensing left ambiguous because of murky status of OpenAI-derived Self-Instruct data">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="What little code available is not very systematically documented">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1/discussions/2#64c29e4c617b36543dedac9a" target="_blank" title="Some info can be gleaned at link but most remains undocumented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4" target="_blank" title="Instructs reader to look up model card for prior 65B Llama1 version">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1" target="_blank" title="Datasheet for RL data only">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API found">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1" target="_blank" title="">Jon Durbin</a></td><td class="llmbase" colspan="3">LLM base: Llama2</td><td class="rlbase" colspan="3">RL base: Airoboros (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/jondurbin" target="_blank" title="Jon Durbin">5.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ChatGLM-6B.yaml" target="_blank" title="data: ChatGLM-6B.yaml">ChatGLM-6B</a></td><td class="partial data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#deployment" target="_blank" title="Some code made available on Github">~</a></td><td class="partial data-cell"><a href="http://doi.org/10.18653/v1/2022.acl-long.26" target="_blank" title="Training data not centrally made available, but described in 2022 ACL paper, appears to be mostly public datasets">~</a></td><td class="open data-cell"><a href="https://huggingface.co/THUDM/chatglm-6b/tree/main" target="_blank" title="Model made available through HuggingFace">✔︎</a></td><td class="closed data-cell"><a and="" are="" bootstrap,="" but="" clearly="" datasets="" feedback="" feedback",="" fine-tuning,="" href="" human="" learning="" none="" of="" reinforcement="" specified."="" supervised="" target="_blank" the="" title="docs mention " used="" wit="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No weights or checkpoints corresponding to the delta of the LLM vs RLHF provided">✘</a></td><td class="open data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE" target="_blank" title="Apache 2.0">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README_en.md" target="_blank" title="Some documentation available, but a lot of code is not commented or explained.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Full details of architecture not specified in a single place">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="partial data-cell"><a href="https://aclanthology.org/2022.acl-long.26/" target="_blank" title="ACL 2022 paper describes the training of the GLM base model, but the RLHF portion is more recent (there is also a related ICLR paper for a newer generation https://openreview.net/forum?id=-Aw0rrrPUF)">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/THUDM/chatglm-6b" target="_blank" title="No modelcard; the HuggingFace modelcard spot is used just as the homepage for the model.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package">✘</a></td><td class="open data-cell"><a href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md#api-deployment" target="_blank" title="API provided through fastapi uvicorn">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a 1t="" 6.2="" able="" about="" and="" answers="" are="" billion="" bootstrap,="" by="" chatglm-6b="" chatgpt,="" chinese="" corpus,="" dialogue.="" english="" feedback="" feedback.="" fine-tuning,="" for="" generate="" href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md" human="" in="" is="" learning="" line="" model="" of="" only="" optimized="" parameters,="" preference.""="" qa="" reinforcement="" similar="" supervised="" supplemented="" target="_blank" technology="" that="" the="" title="From the readme, " to="" tokens="" trained="" uses="" wit="" with="">THUDM</a></td><td class="llmbase" colspan="3">LLM base: GLM (own)</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/THUDM" target="_blank" title="THUDM">5.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/mistral-7B.yaml" target="_blank" title="data: mistral-7B.yaml">Mistral 7B-Instruct</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="repository provides 'minimal code to run our 7B model'">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information provided on pretraining data">✘</a></td><td class="open data-cell"><a href="https://github.com/mistralai/mistral-src#download-the-model" target="_blank" title="Base LLM model made available for download">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank" title="No information provided expect that instruction tuning is done using an unspecified 'variety of publicly available conversation datasets'">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/tree/main" target="_blank" title="Instruct version of the model made available but no information on fine-tuning procedure provided">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/README.md" target="_blank" title="Apache 2.0">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="the little code that is available is uncommented and undocumented">✘</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="Some information on architecture provided in github repo">~</a></td><td class="partial data-cell"><a href="http://arxiv.org/abs/2310.06825" target="_blank" title="Preprint rehashes marketing blurbs also given in blog and provides no details about pretraining datasets, instruction tuning datasets, or fine-tuning process, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer reviewed paper available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available, HuggingFace modelcard just points to a corporate blog post">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet available">✘</a></td><td class="partial data-cell"><a href="https://docs.mistral.ai/quickstart/" target="_blank" title="Docker image shared on github">~</a></td><td class="open data-cell"><a href="https://docs.mistral.ai/api" target="_blank" title="API specification provided by vLLM">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank" title="">Mistral AI</a></td><td class="llmbase" colspan="3">LLM base: unclear</td><td class="rlbase" colspan="3">RL base: unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://mistral.ai/" target="_blank" title="Mistral AI">5.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/wizardlm-7B-V1.yaml" target="_blank" title="data: wizardlm-7B-V1.yaml">WizardLM-7B</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Fast-evolving repository contains WizardLM code">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Based on LLaMA, which is claimed to be public but nowhere exactly documented.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on LLaMA weights, which are not openly available though a leaked versions is in wide circulation.">✘</a></td><td class="open data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM#training-data" target="_blank" title="The Evol-Instruct dataset contains 70k instruction-following sequences generated from Evol-Instruct">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0/tree/main" target="_blank" title="Model weights offered as a delta to LLaMA">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE" target="_blank" title="Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0">~</a></td><td class="partial data-cell"><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardLM" target="_blank" title="Code is only partially documented, not clearly versioned, and appears to be in flux.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Architecture described in preprint and partly accessible in code repository">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2304.12244" target="_blank" title="Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper or data audit found">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank" title="Model card is only a placeholder and generates an error (missing yaml metadata)">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" target="_blank" title="Dataset card for Evol-Instruct generates an error">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/nlpxucan/WizardLM" target="_blank" title="Empowering Large Pre-Trained Language Models to Follow Complex Instructions">Microsoft &amp; Peking University</a></td><td class="llmbase" colspan="3">LLM base: LLaMA-7B</td><td class="rlbase" colspan="3">RL base: Evol-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/nlpxucan" target="_blank" title="Microsoft &amp; Peking University">5.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/mistral-nemo.yaml" target="_blank" title="data: mistral-nemo.yaml">Mistral NeMo Instruct</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-inference" target="_blank" title="repository provides 'minimal code to run our models'">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information provided on pretraining data">✘</a></td><td class="open data-cell"><a href="https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-base-2407.tar" target="_blank" title="Base LLM model made available for download">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank" title="No information provided expect that instruction tuning is done using an unspecified 'variety of publicly available conversation datasets'">✘</a></td><td class="partial data-cell"><a href="https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-instruct-2407.tar" target="_blank" title="Instruct version of the model made available but no information on fine-tuning procedure provided">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/README.md" target="_blank" title="Apache 2.0">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/mistralai/mistral-inference" target="_blank" title="the little code that is available is uncommented and undocumented">✘</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-inference" target="_blank" title="Some information on architecture provided in github repo and in release blogpost">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer reviewed paper available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet available">✘</a></td><td class="partial data-cell"><a href="https://docs.mistral.ai/quickstart/" target="_blank" title="Docker image shared on github">~</a></td><td class="open data-cell"><a href="https://docs.mistral.ai/api" target="_blank" title="API specification provided by vLLM">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://mistral.ai/news/mistral-nemo/" target="_blank" title="">Mistral AI</a></td><td class="llmbase" colspan="3">LLM base: Mistral NeMo</td><td class="rlbase" colspan="3">RL base: unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://mistral.ai/" target="_blank" title="Mistral AI">5.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/qwen-1.5-chat.yaml" target="_blank" title="data: qwen-1.5-chat.yaml">Qwen 1.5</a></td><td class="partial data-cell"><a href="https://github.com/QwenLM/Qwen1.5/" target="_blank" title="Repository provides sparse source code and some examples for SFT">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Pretraining data not specified or documented.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/Qwen/Qwen1.5-72B/tree/main" target="_blank" title="Also available in smaller model sizes">✔︎</a></td><td class="closed data-cell"><a href="https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html" target="_blank" title="Data not specified or documented. Some example code in repo provides directions but no details.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/Qwen/Qwen1.5-72B-Chat/tree/main" target="_blank" title="Also available in smaller model sizes">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Qianwen License">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Repository is fairly well-documented.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="No clear description of architecture found.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Model card on HF only serves as a pointer to the model, no actual info provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet.">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="No specific package provided but integrates well with many widely used packages">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Available through various APIs">✔︎</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://qwenlm.github.io/blog/qwen1.5/" target="_blank" title="This is based on the 72B version, the largest of 8 available model sizes.">Alibaba Cloud</a></td><td class="llmbase" colspan="3">LLM base: QwenLM</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Alibaba Cloud">5.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/stablevicuna.yaml" target="_blank" title="data: stablevicuna.yaml">StableVicuna-13B</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Some elements of the code made available through HuggingFace">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="Based on LLaMA whose pretraining data has nowhere been disclosed or documented.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta#apply-delta-weights" target="_blank" title="Model not functional out of the box as weights require a delta computation. From the docs 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="From the documentation 'The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/discussions/7" target="_blank" title="The HuggingFace community page has an open question for release of the RL model">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="CC-BY-NC-SA-4.0. License for LLaMA is more murky, hence partial. As they say 'License for the base LLaMA model's weights is Meta's non-commercial bespoke license.'">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta/tree/main" target="_blank" title="Code is minimally documented and deployment requires non-trivial configuration, e.g. 'StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights.'">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Architecture is described in scattered places, but there is no clear and exhaustive overview.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2302.13971" target="_blank" title="Preprint covers only the LLaMA base model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/lmsys/vicuna-13b-delta-v0" target="_blank" title="Model card provides some information but is not fully worked out as recommended in model card literature.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="partial data-cell"><a href="https://github.com/lm-sys/FastChat/tree/main#api" target="_blank" title="Addressable via FastChat / HuggingFace API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/CarperAI/stable-vicuna-13b-delta" target="_blank" title="StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets">CarperAI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: OASST1 (human), GPT4All (human), Alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://carper.ai" target="_blank" title="CarperAI">5.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Falcon-40B-instruct.yaml" target="_blank" title="data: Falcon-40B-instruct.yaml">Falcon-40B-instruct</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" is="" open="" source"="" target="_blank" title="No source code shared, even though the term " used."="">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" target="_blank" title="From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (&gt;80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct/tree/main" target="_blank" title="Model weights available through HuggingFace library">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/project-baize/baize-chatbot" target="_blank" title="RL data inherited from Baize but provenance not well-documented. From the documentation 'Falcon-40B-Instruct was finetuned on a 150M tokens from Baize mixed with 5% of RefinedWeb data.'">~</a></td><td class="closed data-cell"><a href="https://github.com/project-baize/baize-chatbot#v1" target="_blank" title="No RL weights or checkpoints made available">✘</a></td><td class="open data-cell"><a href="" target="_blank" title="First release came with a legally murky license that was swiftly criticised and now generates a 404. Current documentation 'Falcon-40B-Instruct is made available under the Apache 2.0 license.'">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code found, therefore no documentation found.">✘</a></td><td class="partial data-cell"><a 40b="" a="" and="" baize.""="" based="" built="" by="" causal="" decoder-only="" falcon-40b="" falcon-40b-instruct="" finetuned="" href="" is="" mixture="" model="" of="" on="" parameters="" target="_blank" tii="" title="Architecture sketched on HuggingFace as ">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2306.01116" target="_blank" title="Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper known.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no datasheet available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no package.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="There is no API, and HuggingFace inference API is disabled for this model.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" title="">Technology Innovation Institute</a></td><td class="llmbase" colspan="3">LLM base: Falcon 40B</td><td class="rlbase" colspan="3">RL base: Baize (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://falconllm.tii.ae" target="_blank" title="Technology Innovation Institute">4.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/ultraLM.yaml" target="_blank" title="data: ultraLM.yaml">UltraLM</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on Llama2, which means pretraining data is nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="open data-cell"><a href="https://huggingface.co/datasets/openbmb/UltraFeedback" target="_blank" title="UltraFeedback dataset made available along with model release">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b-v2.0" target="_blank" title="Online materials appear to be in flux and several HuggingFace links generate 404 errors, hence partial">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Architecture sketched in online materials.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2310.01377" target="_blank" title="Preprint describes creation of UltraFeedback dataset but also offers some detail on training and architecture of UltraRM models">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/openbmb/UltraLM-13b#model-details" target="_blank" title="Model card on HuggingFace available for 13b (Llama1) model, but not for newer releases, hence partial">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/openbmb/UltraFeedback" target="_blank" title="Datasheet available for RLHF portion, but not for Llama2-based pretraining data, hence partial">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="API not provided and model too big for HuggingFace inference API">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/openbmb/UltraRM-13b" target="_blank" title="">OpenBMB</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: UltraFeedback (part synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://www.openbmb.org/" target="_blank" title="OpenBMB">4.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/yi-chat.yaml" target="_blank" title="data: yi-chat.yaml">Yi 34B Chat</a></td><td class="partial data-cell"><a href="https://github.com/01-ai/Yi" target="_blank" title="repository contains some code for demos and for instruction tuning, but only sparse examples of code for initial training and model architecture">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Training data for base model undocumented, though preprint mentions CommonCrawl as one source">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/01-ai/Yi-34B/tree/main" target="_blank" title="Base model weights shared via HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/datasets/BAAI/COIG" target="_blank" title="Model docs mention COIG but only as an example. Code also shows rm-static is also used in instruction tuning, but not shared.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/01-ai/Yi-34B-Chat/tree/main" target="_blank" title="Instruction-tuned weights shared via HuggingFace">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Model code licensed under Apache 2.0, model weights licensed under Llama-inspired community license with limitations">~</a></td><td class="closed data-cell"><a href="https://github.com/01-ai/Yi/" target="_blank" title="Documentation only describes how to deploy, training and fine-tuning code not available or where available not commented or documented">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Model architecture only described in the most general terms.">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2403.04652" target="_blank" title="Preprint provides some info on pretraining data (CommonCrawl) but none on instruction tuning dataset.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Model card used for advertising and navigation purposes, does not follow general model card specifications.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No data sheets found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="no package found">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="limited access via HF API, otherwise gated and commercially available">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/01-ai/Yi-34B-Chat" target="_blank" title="The Yi series models are the next generation of open-source large language models trained from scratch by 01.AI. Targeted as a bilingual language model and trained on 3T multilingual corpus.">01.AI</a></td><td class="llmbase" colspan="3">LLM base: Yi 34B</td><td class="rlbase" colspan="3">RL base: unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://www.01.ai/" target="_blank" title="01.AI">4.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/koala.yaml" target="_blank" title="data: koala.yaml">Koala 13B</a></td><td class="open data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across projects and repositories">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/koala_data_pipeline" target="_blank" title="Repo contains data pipeline for preprocessing. Based on LLaMA which is said to be based on 'publicly available datasets' which are not made directly available.">~</a></td><td class="partial data-cell"><a href="https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl" target="_blank" title="Model weights only made available as a diff against LLaMA. OpenLLaMA provides a possible alternative?">~</a></td><td class="partial data-cell"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/#datasets-and-training" target="_blank" title="Datasets described in blog post but not all made available">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RL weights not made available separately">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/young-geng/koala#license" target="_blank" title="Licensing is 'subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT'">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Code scattered across various repositories and not systematically documented.">~</a></td><td class="partial data-cell"><a href="https://github.com/young-geng/EasyLM" target="_blank" title="Architecture visually illustrated and described in some detail.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No systematic data sheet available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/" target="_blank" title="From the documentation 'Koala is fine-tuned on freely available interaction data scraped from the web, but with a specific focus on data that includes interaction with highly capable closed-source models such as ChatGPT.'">BAIR</a></td><td class="llmbase" colspan="3">LLM base: LLaMA 13B</td><td class="rlbase" colspan="3">RL base: HC3, ShareGPT, alpaca (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://bair.berkeley.edu/" target="_blank" title="BAIR">4.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-3.1.yaml" target="_blank" title="data: llama-3.1.yaml">Llama 3.1</a></td><td class="partial data-cell"><a href="https://github.com/meta-llama/llama3" target="_blank" title="Repository only offers code for inference pipeline">~</a></td><td class="closed data-cell"><a from="" href="" obtained="" target="_blank" the="" title="Data nowhere disclosed or documented, and described in the llama 3.1 paper as " web""="">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B" target="_blank" title="Inspecting the training weights requires signing Meta Llama 3.1's bespoke 'community license', not an OSI recognised open license">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information available on instruction-tuning.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Inspecting the training weights requires signing Meta Llama 3.1's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B" target="_blank" title="Inspecting the training weights requires signing Meta Llama 3.1's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code provide only model architecture and an inferencing pipeline examples; some files are documented.">~</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank" title="Architecture described in paper, energy consumtion and environmental impact disclosed in model card, but not in paper; training process not fully documented">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint or any other scientific documentation available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper available.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B" target="_blank" title="There is a model card, but it does not disclose the training process">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided at all">✘</a></td><td class="open data-cell"><a href="https://pypi.org/project/llama-models/" target="_blank" title="Package is provided in Pypi">✔︎</a></td><td class="partial data-cell"><a href="https://www.meta.ai/?utm_source=ai_meta_site&amp;utm_medium=web&amp;utm_content=AI_nav&amp;utm_campaign=April_moment" target="_blank" title="API only available behind a privacy-defying signup form">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B" target="_blank" title="">Facebook Research</a></td><td class="llmbase" colspan="3">LLM base: Meta Llama 3</td><td class="rlbase" colspan="3">RL base: Meta, undocumented</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/facebookresearch" target="_blank" title="Facebook Research">4.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/mixtral-8x7B-instruct.yaml" target="_blank" title="data: mixtral-8x7B-instruct.yaml">Mixtral 8x7B Instruct</a></td><td class="closed data-cell"><a href="" target="_blank" title="No training code for Mixtral 8x7B made available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information provided on pretraining data">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/tree/main" target="_blank" title="Weights shared via torrent and via HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/tree/main" target="_blank" title="No information provided online or in the preprint">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/tree/main" target="_blank" title="Instruct version of model weights made available but no information on fine-tuning procedure provided">~</a></td><td class="open data-cell"><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/blob/main/README.md" target="_blank" title="Apache 2.0">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="the little code that is available is uncommented and undocumented, and relates to a reference implementation of the 7B version">✘</a></td><td class="partial data-cell"><a href="https://github.com/mistralai/mistral-src" target="_blank" title="Some information on architecture provided in github repo and in preprint">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2401.04088" target="_blank" title="Preprint describes mixture of experts and evalutaion but provides no details about pretraining datasets, instruction tuning datasets, or fine-tuning process, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer reviewed paper available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card available, HuggingFace modelcard just points to a corporate blog post">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet available">✘</a></td><td class="partial data-cell"><a href="https://docs.mistral.ai/quickstart/" target="_blank" title="Docker image shared on github">~</a></td><td class="closed data-cell"><a href="https://console.mistral.ai/" target="_blank" title="Only paid API access available">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" target="_blank" title="">Mistral AI</a></td><td class="llmbase" colspan="3">LLM base: Mistral</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://mistral.ai" target="_blank" title="Mistral AI">4.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/StableBeluga2.yaml" target="_blank" title="data: StableBeluga2.yaml">Stable Beluga 2</a></td><td class="closed data-cell"><a href="" target="_blank" title="No repository with open code related to training, fine-tuning or evaluation found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Pretraining data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Says: 'Stable Beluga 2 is trained on our internal Orca-style dataset', 'created synthetically using high-quality instructions' from COT Submix Original, NIV2 Submix Original, FLAN 2021 Submix Original, T0 Submix Original.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2/tree/main" target="_blank" title="Instruction-tuned model weights available">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt" target="_blank" title="Usage requires signing StabilityAI's bespoke 'Stable Beluga Non-Commercial Community License', not an OSI recognised open license">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code on HuggingFace only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2#model-details" target="_blank" title="Says 'Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset'.">~</a></td><td class="partial data-cell"><a href="https://huggingface.co/papers/2306.02707" target="_blank" title="Preprint from Microsoft describes Orca method of finetuning using GPT4-derived synthetic data, but no details of this particular architecture">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/StableBeluga2" target="_blank" title="There is a model card, but it provides only a minimum of detail">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="model too large to run on HuggingFace free inference API">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/stabilityai/StableBeluga2" target="_blank" title="">Stability AI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Orca-style (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://stability.ai/" target="_blank" title="Stability AI">4.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/alpaca.yaml" target="_blank" title="data: alpaca.yaml">Stanford Alpaca</a></td><td class="open data-cell"><a href="" target="_blank" title="">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Based on LLaMA, whose pretraining data is nowhere disclosed or documented.">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="LLaMA based, copyright status unclear">~</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model.">~</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" title="LLaMA based">~</a></td><td class="closed data-cell"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform" target="_blank" title="Pegged to LLaMA licence agreement">✘</a></td><td class="partial data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" title="Insofar as code is made available it is fairly well documented">~</a></td><td class="open data-cell"><a href="https://github.com/tatsu-lab/stanford_alpaca#fine-tuning" target="_blank" title="Fair bit of documentation available on github repository">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found; uses the release-by-blogpost playbook">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed work found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No data sheet found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" title="project_notes">Stanford University CRFM</a></td><td class="llmbase" colspan="3">LLM base: LLaMA</td><td class="rlbase" colspan="3">RL base: Self-Instruct (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://crfm.stanford.edu/" target="_blank" title="Stanford University CRFM">4.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Falcon-180B-chat.yaml" target="_blank" title="data: Falcon-180B-chat.yaml">Falcon-180B-chat</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="No source code shared anywhere. The over 200 github repositories of TII appear to include no LLM or Falcon related code.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" target="_blank" title="From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (&gt;80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.">~</a></td><td class="partial data-cell"><a acceptable="" href="https://huggingface.co/tiiuae/falcon-180B" policy""="" target="_blank" title="requires signing up and accepting " use="">~</a></td><td class="partial data-cell"><a a="" airoboros"."="" and="" finetuned="" href="https://github.com/project-baize/baize-chatbot" mixture="" of="" on="" platypus="" target="_blank" title="No details provided beyond " ultrachat,="">~</a></td><td class="partial data-cell"><a acceptable="" href="https://huggingface.co/tiiuae/falcon-180B-chat/tree/main" policy"."="" target="_blank" title="No RL weights or checkpoints made available; fine-tuned 'chat' model only available after signing " use="">~</a></td><td class="closed data-cell"><a acceptable="" href="https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt" policy"."="" target="_blank" title="Released under Falcon 180B TII license (not OSI approved) and a separate " use="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code available, so no code documented.">✘</a></td><td class="partial data-cell"><a "optimized="" and="" available."="" but="" causal="" decoder-only"="" details="" few="" for="" href="https://huggingface.co/tiiuae/falcon-180B-chat#model-architecture-and-objective" inference",="" target="_blank" title="Architecture described as  " very="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2306.01116" target="_blank" title="Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper known.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no datasheet available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="There is no package.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="There is no API, and HuggingFace inference API is disabled for this model.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/tiiuae/falcon-180B-chat" target="_blank" title="Falcon-180B-Chat is a 180B parameters causal decoder-only model built by TII based on Falcon-180B and finetuned on a mixture of Ultrachat, Platypus and Airoboros.">Technology Innovation Institute</a></td><td class="llmbase" colspan="3">LLM base: Falcon 180B</td><td class="rlbase" colspan="3">RL base: OpenPlatypus, Ultrachat, Airoboros (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://falconllm.tii.ae" target="_blank" title="Technology Innovation Institute">3.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/gemma-instruct.yaml" target="_blank" title="data: gemma-instruct.yaml">Gemma 7B Instruct</a></td><td class="partial data-cell"><a href="https://github.com/google-deepmind/gemma" target="_blank" title="No pre-training or instructing-tuning code made available. Some developer tools available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No details provided on pre-training data.">✘</a></td><td class="partial data-cell"><a href="https://www.kaggle.com/models/keras/gemma/frameworks/keras/variations/gemma_7b_en" target="_blank" title="Base model weights shared via Kaggle, requires privacy-defying access request.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Documentation says 'These versions of the model are trained with human language interactions and can respond to conversational input, similar to a chat bot.' ">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Instruction-tuned model weights shared via Kaggle, requires privacy-defying access request">~</a></td><td class="closed data-cell"><a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?pli=1" target="_blank" title="Bespoke Gemma Community License Agreement and restrictive Terms of Use, neither open in the sense of OSI. Only Inference code shared under Apache 2.0.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No pretraining or finetuning code found. No code documentation except for deployment of the open weights.">✘</a></td><td class="partial data-cell"><a href="https://www.kaggle.com/models/google/gemma" target="_blank" title="Architecture described in very general terms in model card">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2403.08295" target="_blank" title="Preprint released Apr 2024 details architecture and evaluation, but provides no information on pre-training, instruction tuning and SFT datasets">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="open data-cell"><a href="https://www.kaggle.com/models/google/gemma" target="_blank" title="Model card on Kaggle provides some detail on model internals and evaluation">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet found, pre-training and instruction-tuning data nowhere specified.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package provided, access is gated through Kaggle, Vertex Model Garden, Google Cloud">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No API provided, access gated through Kaggle, Vertex Model Garden, Google Cloud">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://ai.google.dev/gemma/docs" target="_blank" title="Model weights and developer tools">Google DeepMind</a></td><td class="llmbase" colspan="3">LLM base: Gemma</td><td class="rlbase" colspan="3">RL base: Unspecified</td><td colspan="7"></td><td class="source-link"><a href="https://ai.google.dev/gemma/docs" target="_blank" title="Google DeepMind">3.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/orca_2.yaml" target="_blank" title="data: orca_2.yaml">Orca 2</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code made available anywhere for data curation, training, fine-tuning, or evaluation">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Llama pretraining Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Instruction-tuning and explanation-tuning data described in preprint, but not disclosed or made available">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/microsoft/Orca-2-7b" target="_blank" title="7B and 13B finetunes of Llama2 made available through HuggingFace">✔︎</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" target="_blank" title="Licensed under Microsoft Research License and Meta's bespoke community license, neither OSI recognised">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Architecture sketched in corporate preprints, though many details missing.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2311.11045.pdf" target="_blank" title="Corporate preprint from Microsoft Research has some detail on instruction-tuning, explanation-tuning and evaluation">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/microsoft/Orca-2-7b" target="_blank" title="Model card serves as landing page more than careful model documentation">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Model too large to load on HuggingFace inference library">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://www.microsoft.com/en-us/research/project/orca/" target="_blank" title="This file applies to Orca 2 7B and 13B, both fine-tunes of corresponding Llama2 base models.">Microsoft Research</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: FLAN, Math, undisclosed (synthetic)</td><td colspan="7"></td><td class="source-link"><a href="https://www.microsoft.com/en-us/research/project/orca/" target="_blank" title="Microsoft Research">3.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/command-r.yaml" target="_blank" title="data: command-r.yaml">Command R+</a></td><td class="closed data-cell"><a href="" target="_blank" title="No codebase available to study or adjust model architecture, training, or inner workings.">✘</a></td><td class="closed data-cell"><a href="https://docs.cohere.com/docs/data-statement" target="_blank" title="No documentation, listing or audit of pre-training data available. Cohere itself identifies it as coheretext-filtered and gives the size as 200Gb.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No checkpoint or model prior to SFT and instruction-tuning made available">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/collections/CohereForAI/aya-datasets-660415741bd4852f01c81c77" target="_blank" title="Aya Collection (Aya Open Science initiative) is a multilingual collection of 513 million instances of promts and completions including instruction-style templates.">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/CohereForAI/c4ai-command-r-v01/tree/main" target="_blank" title="Fine-tuned model weights made available for download">✔︎</a></td><td class="partial data-cell"><a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy" target="_blank" title="Licensed under CC-BY-NC and requires agreeing to C4AI acceptable use policy">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code available, so no documentation of code.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Architecture only sparsely documented.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint appears to be made available at this time.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper known to document the Cohere Command R+ model or architecture.">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/CohereForAI/c4ai-command-r-v01-4bit" target="_blank" title="Model card on HF document some aspects but provides no data on training data, instruction-tuning methods">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No separate package available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="API access available only when signing up.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/CohereForAI/c4ai-command-r-v01" target="_blank" title="">Cohere AI</a></td><td class="llmbase" colspan="3">LLM base: </td><td class="rlbase" colspan="3">RL base: Aya Collection</td><td colspan="7"></td><td class="source-link"><a href="https://cohere.com" target="_blank" title="Cohere AI">3.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml" target="_blank" title="data: llama-2-chat.yaml">LLaMA2 Chat</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/tree/main" target="_blank" title="Repository only offers 'a minimal example to load Llama 2 models and run inference'; no training, fine-tuning, evaluation code made available">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF data including 1 million Meta-specific tuning prompts not made available (even as it incorporates some open RLHF datasets)">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Architecture sketched in preprint, though many details missing.">~</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Corporate preprint quite some detail on pretraining, RLHF, and safety measures but none on datasets.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md" target="_blank" title="There is a model card, but it provides the absolute minimum of detail, and none whatsoever on training data.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API only available behind a privacy-defying signup form">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://ai.meta.com/resources/models-and-libraries/llama/" target="_blank" title="">Facebook Research</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Meta, StackExchange, Anthropic</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/facebookresearch" target="_blank" title="Facebook Research">3.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/nanbeige-chat.yaml" target="_blank" title="data: nanbeige-chat.yaml">Nanbeige2-Chat</a></td><td class="open data-cell"><a href="https://github.com/Nanbeige/Nanbeige" target="_blank" title="github repo contains sparse but clear code for training, tuning, and inference">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information on pre-training datasets except a claim of 4.5T tokens. Request for information on HF community was closed without comment.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Base model not shared">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat/discussions/2#6621e15a4d17641cf788cbd5" target="_blank" title="No information on finetuning and DPO datasets. Some information provided on request (see link), but official documentation not updated.">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat/tree/main" target="_blank" title="Model weights for finetuned model shared">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Apache 2.0 but commercial use requires signup and an additional community license">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No documentation of the codebase">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Architecture not clearly specified">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No paper found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model card found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No datasheet found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No package found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/spaces/Nanbeige/Nanbeige-Plus-Chat-v0.1" target="_blank" title="No API, but HuggingFace space available">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/Nanbeige/Nanbeige2-8B-Chat" target="_blank" title="Comes in 8B and 16B versions">Nanbeige LLM lab</a></td><td class="llmbase" colspan="3">LLM base: Unknown</td><td class="rlbase" colspan="3">RL base: Unknown</td><td colspan="7"></td><td class="source-link"><a href="https://huggingface.co/Nanbeige" target="_blank" title="Nanbeige LLM lab">3.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-3-instruct.yaml" target="_blank" title="data: llama-3-instruct.yaml">Llama 3 Instruct</a></td><td class="closed data-cell"><a href="https://github.com/meta-llama/llama3" target="_blank" title="Repository only offers minimal code">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a release blog post">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No information available on instruction-tuning.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/tree/main" target="_blank" title="Even inspecting the model requires signing Meta Llama 3's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code only covers minimal examples; no documentation available.">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" target="_blank" title="Architecture sketched in glossy blog post.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint or any other scientific documentation available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper available.">✘</a></td><td class="partial data-cell"><a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" target="_blank" title="There is a model card, but it provides the absolute minimum of detail, and none whatsoever on training data.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API only available behind a privacy-defying signup form">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6" target="_blank" title="">Facebook Research</a></td><td class="llmbase" colspan="3">LLM base: Meta Llama 3</td><td class="rlbase" colspan="3">RL base: Meta, undocumented</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/facebookresearch" target="_blank" title="Facebook Research">2.5</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/solar-70B.yaml" target="_blank" title="data: solar-70B.yaml">Solar 70B</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository found">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No RLHF datasets specified or shared, docs say 'Orca-style dataset, Alpaca-style dataset'">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit/tree/main" target="_blank" title="Finetuned checkpoints only shared through CC-BY-NC">~</a></td><td class="closed data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit#model-details" target="_blank" title="Meta Community License for base model, and CC-BY-NC 4.0 for fine-tuned model weights">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="HuggingFace code only comprises configuration json; no documentation available.">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="Precise architecture, training, fine-tuning procedures not given.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No preprint or any form of scientific docuentation found.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper found">✘</a></td><td class="partial data-cell"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="HuggingFace model card used mostly as advertising, omits many details on training, fine-tuning, evaluation.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API only available by signing up for 'private LLM' service">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" title="HuggingFace profile says 'Solar is a great example of the progress enabled by open source.'">Upstage AI</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: Orca-style, Alpaca-style</td><td colspan="7"></td><td class="source-link"><a href="https://en.upstage.ai/" target="_blank" title="Upstage AI">2.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/Xwin-LM.yaml" target="_blank" title="data: Xwin-LM.yaml">Xwin-LM</a></td><td class="closed data-cell"><a :"="" code'"="" href="https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1" release="" source="" target="_blank" the="" title="HuggingFace page notes 'to do ">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Data nowhere disclosed or documented, and described only in the vaguest terms in a corporate preprint released by Meta">✘</a></td><td class="partial data-cell"><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" title="Download only after requesting access; requires signing a consent form">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="RLHF data for Llama includes 1 million Meta-specific tuning prompts not made available, no other details known about RLHF and alignment tuning added by Xwin-LM">✘</a></td><td class="closed data-cell"><a href="https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1/tree/main" target="_blank" title="Downloadable model presumably includes RLHF tuning but no documentation available">✘</a></td><td class="closed data-cell"><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" target="_blank" title="Usage requires signing Meta's bespoke 'community license', not an OSI recognised open license">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No documentation available.">✘</a></td><td class="closed data-cell"><a crucial="" href="https://github.com/Xwin-LM/Xwin-LM#news" plays="" rlhf="" role""="" target="_blank" title="No information available beyond that it is based on Llama and ">✘</a></td><td class="closed data-cell"><a (stay="" coming="" href="https://github.com/Xwin-LM/Xwin-LM#news" soon="" target="_blank" title="No preprint available; " tuned)""="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper available">✘</a></td><td class="closed data-cell"><a advertise="" available."="" but="" card"="" details="" href="https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1" model="" model,="" no="" target="_blank" title="HuggingFace " to="" used="">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Datasheet not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Package not provided">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="API available through vllm and HuggingFace">~</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1" target="_blank" title="Xwin-LM aims to develop and open-source alignment technologies for large language models">Xwin-LM</a></td><td class="llmbase" colspan="3">LLM base: LLaMA2</td><td class="rlbase" colspan="3">RL base: unknown</td><td colspan="7"></td><td class="source-link"><a href="https://github.com/Xwin-LM" target="_blank" title="Xwin-LM">1.0</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/chatgpt.yaml" target="_blank" title="data: chatgpt.yaml">ChatGPT</a></td><td class="closed data-cell"><a href="https://chat.openai.com/" target="_blank" title="OpenAI has not released any source code related to ChatGPT">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released or documented any of the source training data">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released model weights for GPT3.5">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released the instruction-tuning data">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released details about RLHF models weights">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="None of the code is open sourced; license unknown.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Code is not available, documentation level unknown.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not clearly documented the LLM+RLHF architecture or its evaluation.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2203.02155" target="_blank" title="Preprint describes only the instruction-tuning method; no further papers available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="No peer-reviewed paper documenting this system is available.">✘</a></td><td class="closed data-cell"><a href="https://github.com/openai/gpt-3/blob/master/model-card.md" target="_blank" title="No modelcard is available for GPT3.5. The linked model card is for GPT3 is dated Sept 2020.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="OpenAI has not released a datasheet or any other documentation or evaluation of source data.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Some third party packages exist; support for them is contingent on OpenAI's whims.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="API access is only available for commercial users.">✘</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://chat.openai.com/" target="_blank" title="NA">OpenAI</a></td><td class="llmbase" colspan="3">LLM base: GPT 3.5</td><td class="rlbase" colspan="3">RL base: Instruct-GPT</td><td colspan="7"></td><td class="source-link"><a href="https://openai.com/" target="_blank" title="OpenAI">0.5</a></td></tr>
</tbody>
</table>
</div>
<p id="table-guide"><em>How to use this table.</em> Every cell records a three-level openness judgement (<span class="openness open"><strong>✔︎</strong> open</span>, <span class="openness partial"><strong>~</strong> partial</span> or <span class="openness closed"><strong>✘</strong> closed</span>) with a direct link to the available evidence; on hover, the cell will display the notes we have on file for that judgement. The name of each project is a direct link to source data. The table is sorted by cumulative openness, where <strong>✔︎</strong> is 1, <strong>~</strong> is 0.5 and <strong>✘</strong> is 0 points. Note that RL may refer to RLHF or other forms of fine-tuning aimed at fostering instruction-following behaviour.</p>
<h2>Why is openness important?</h2>
<p>Open research is the lifeblood of cumulative progress in science and engineering. Openness is key for fundamental research, for fostering critical computational literacy, and for making informed choices for or against deployment of instruction-tuned LLM architectures. The closed &amp; proprietary nature of ChatGPT and kin makes them fundamentally unfit for responsible use in research and education.</p>
<p>Open alternatives provide ways to build reproducible workflows, chart resource costs, and lessen reliance on corporate whims. One aim of our work here is to provide tools to track openness, transparency and accountability in the fast-evolving landscape of instruction-tuned text generators. Read more in the <a href="https://dl.acm.org/doi/10.1145/3571884.3604316" target="_blank">paper</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>) or <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/" target="_blank">contribute to the repo</a>.</p>
<h2>TL;DR</h2>
<p>Our paper makes the following contributions:</p>
<ul>
<li>We review the risks of relying on proprietary software</li>
<li>We review best practices for open, transparent and accountable 'AI'</li>
<li>We find over 40 ChatGPT alternatives at varying degrees of openness, development and documentation</li>
<li>We argue that tech is never a <em>fait accompli</em> unless we make it so, and that openness enables critical computational literacy</li>
</ul>
<p>We find the following recurrent patterns:</p>
<ul>
<li>Many projects inherit data of dubious legality</li>
<li>Few projects share the all-important instruction-tuning</li>
<li>Preprints are rare, peer-reviewed papers even rarer</li>
<li>Synthetic instruction-tuning data is on the rise, with unknown consequences that are in need of research</li>
</ul>
<p>We conclude as follows:</p>
<blockquote id="conclusion">Openness is not the full solution to the scientific and ethical challenges of conversational text generators. Open data will not mitigate the harmful consequences of thoughtless deployment of large language models, nor the questionable copyright implications of scraping all publicly available data from the internet. However, openness does make original research possible, including efforts to build reproducible workflows and understand the fundamentals of instruction-tuned LLM architectures. Openness also enables checks and balances, fostering a culture of accountability for data and its curation, and for models and their deployment. We hope that our work provides a small step in this direction.
  </blockquote>
<h2>Papers</h2>
<p>Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>. July 19-21, Eindhoven. doi: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>).</p>
<p>Andreas Liesenfeld and Mark Dingemanse. 2024. Rethinking open source generative AI: open washing and the EU AI Act. In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)</em>. Association for Computing Machinery, New York, NY, USA, 1774–1787. doi: <a href="https://doi.org/10.1145/3630106.3659005" target="_blank">10.1145/3630106.3659005</a></p>
</div><!-- #content -->
<div id="footer">
<p>We gratefully acknowledge funding from the Dutch Research Council for the project <em><a href="https://markdingemanse.net/elpaco" target="_blank">Elementary Particles of Conversation</a></em> (016.vidi.185.205), and support from the CLS Humanities Lab (<a href="https://github.com/timjzee/" target="_blank">timjzee</a>). Check out the newest iteration at <a href="https://osai-index.eu/">osai-index.eu</a>.</p>
<p class="copyright">Website &amp; code © 2023-2024 by the authors. If you find any of this useful, our papers provides the canonical and most durable citation.</p>
<p id="build-time">Table last built on 2025-03-11 at 16:21 UTC</p>
</div>
</body>
</html>
"
